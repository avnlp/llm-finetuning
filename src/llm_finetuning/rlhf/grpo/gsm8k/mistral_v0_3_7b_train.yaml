# Mistral 7B v0.3 Configuration
model:
  name: "unsloth/mistral-7b-instruct-v0.3-bnb-4bit"
  max_seq_length: 1024
  quantization:
    load_in_4bit: true
  lora:
    r: 32
    alpha: 32
    dropout: 0
    target_modules:
      - "q_proj"
      - "k_proj"
      - "v_proj"
      - "o_proj"
      - "gate_proj"
      - "up_proj"
      - "down_proj"
    use_gradient_checkpointing: "unsloth"
    random_state: 3407

dataset:
  name: "openai/gsm8k"
  config: "main"
  split: "train"
  preprocessing:
    system_prompt: |
      Respond in the following format:
      <reasoning>
      ...
      </reasoning>
      <answer>
      ...
      </answer>
    answer_extraction: "extract_hash_answer"

training:
  type: "GRPO"
  config:
    learning_rate: 5e-6
    adam_beta1: 0.9
    adam_beta2: 0.99
    weight_decay: 0.1
    warmup_ratio: 0.1
    lr_scheduler: "cosine"
    optim: "paged_adamw_8bit"
    batch_size: 1
    gradient_accumulation_steps: 1
    max_steps: 250
    save_steps: 250
    logging_steps: 1
    fp16: true
    bf16: false
    max_grad_norm: 0.1
  generation:
    num_generations: 6
    max_prompt_length: 256
    max_completion_length: 768  # 1024 - 256

rewards:
  functions:
    - "xmlcount_reward_func"
    - "soft_format_reward_func"
    - "strict_format_reward_func"
    - "int_reward_func"
    - "correctness_reward_func"

inference:
  sampling_params:
    temperature: 0.8
    top_p: 0.95
    max_tokens: 1024

saving:
  lora_dir: "grpo_saved_lora"
  merged_16bit: true
  merged_4bit: true
  gguf:
    methods: ["q8_0", "f16", "q4_k_m"]

output:
  dir: "outputs"