# Model parameters
model_name: "unsloth/Qwen3-4B-unsloth-bnb-4bit"
max_seq_length: 10240
lora_rank: 32
gpu_memory_utilization: 0.7

# Sampling parameters
temperature: 0.6
top_k: 20
top_p: 0.95
min_p: 0.0
max_new_tokens: 1500

# Training parameters
learning_rate: 5e-6
weight_decay: 0.01
warmup_ratio: 0.1
lr_scheduler_type: "cosine"
optim: "adamw_8bit"
logging_steps: 1
per_device_train_batch_size: 1
gradient_accumulation_steps: 1
num_generations: 4
max_steps: 100
save_steps: 5
report_to: "wandb"
output_dir: "outputs"

# Dataset parameters
dataset_name: "awinml/medqa-context"
split: "train"
